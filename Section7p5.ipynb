{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05fc26",
   "metadata": {},
   "source": [
    "Tomoki has solutions, and solutions manual also has some solutions. I have just added some of my own thoughts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f95d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "import sympy as sym\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1385851",
   "metadata": {},
   "source": [
    "#### Exercise 7.5.4. \n",
    "\n",
    "Let $\\overline{X}$ denote the mean of the random sample $X_1, X_2, \\cdots , X_n$ from a gamma-type distribution with parameters $\\alpha > 0$ and $\\beta = \\theta > 0$. Compute $E[X_1| \\overline{x}]$.\n",
    "\n",
    "Hint: Can you find directly a function $\\psi(\\overline{X})$ of $\\overline{X}$ such that $E[\\psi(\\overline{X})] = \\theta$? Is $E[(X_1|\\overline{x}] = \\psi(\\overline{x})$\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "Gamma distribution is of the regular exponential class so the mean is a sufficient statistic. Hence from Theorem $7.5.2$, it is a complete sufficient statistic. Using Theorem $7.3.1$ (Rao Blackwell Theorem), by substituting $Y_2=X_1$ and $Y_1=\\overline{X}$ in that theorem, we can conclude that $\\psi(\\overline{X}) = E[X_1| \\overline{X}]$ is unbiased so that $E[\\psi(\\overline{X})] = \\theta$. \n",
    "\n",
    "Now consider $\\phi(\\overline{X}) = \\overline{X}/\\alpha$. Since $E[\\phi(\\overline{X})] = \\theta$ as well, from Theorem $7.4.1$, we can conclude that $E[X_1| \\overline{X}] = \\psi(\\overline{X}) = \\overline{X}/\\alpha$ almost surely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b0221",
   "metadata": {},
   "source": [
    "#### Exercise 7.5.5. \n",
    "\n",
    "Let $X$ be a random variable with the pdf of a regular case of the exponential class, given by $f(x;\\theta)=exp[p(\\theta) K(x)+H(x)+q(\\theta)]$, $a<x<b$, $γ<\\theta<\\delta$. Show that $E[K(X)] = −q'(\\theta)/p'(\\theta)$, provided these derivatives exist, by differentiating both members of the equality\n",
    "\n",
    "$$\\int_a^b exp[p(\\theta)K(x) + H(x) + q(\\theta)] dx = 1$$\n",
    "\n",
    "with respect to $\\theta$. By a second differentiation, find the variance of $K(X)$.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "There may be a typo in the problem statement specifically in the definition of $f(x;\\theta)$. It should have $p(\\theta)$ in the expoenent and not $\\theta$. I have assumed $p(\\theta)$ in my solution.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "Define $G(x;\\theta) = p(\\theta) K(x)+H(x)+q(\\theta)$. Then differentiating the integral w.r.t $\\theta$, we get\n",
    "\n",
    "$$\\int_a^b exp[p(\\theta)K(x) + H(x) + q(\\theta)] \\frac{\\partial{G(x;\\theta)}}{\\partial{\\theta}}dx = 0$$ and since\n",
    "\n",
    "$$ \\frac{\\partial{G(x;\\theta)}}{\\partial{\\theta}} = \\frac{\\partial{p(\\theta)}}{\\partial{\\theta}}K(x)+\\frac{\\partial{q(\\theta)}}{\\partial{\\theta}},$$ we get\n",
    "\n",
    "$$\\frac{\\partial{p(\\theta)}}{\\partial{\\theta}} \\int_a^b K(x)exp[p(\\theta)K(x) + H(x) + q(\\theta)] dx = -\\frac{\\partial{q(\\theta)}}{\\partial{\\theta}} \\int_a^b exp[p(\\theta)K(x) + H(x) + q(\\theta)] dx$$\n",
    "\n",
    "i.e $$E(K(X)) = \\int_a^b K(x)exp[p(\\theta)K(x) + H(x) + q(\\theta)] dx = -\\frac{\\partial{q(\\theta)}}{\\partial{\\theta}} \\left( \\frac{\\partial{p(\\theta)}}{\\partial{\\theta}} \\right)^{-1} $$ as desired.\n",
    "\n",
    "Now let \n",
    "\n",
    "$$\\begin{align} p' &=  \\frac{\\partial{p(\\theta)}}{\\partial{\\theta}} \\\\ \n",
    "q' &=  \\frac{\\partial{q(\\theta)}}{\\partial{\\theta}} \\\\\n",
    "p'' &=  \\frac{\\partial^2{p(\\theta)}}{\\partial{\\theta}^2} \\\\ \n",
    "q'' &=  \\frac{\\partial^2{q(\\theta)}}{\\partial{\\theta}^2}.\n",
    "\\end{align}$$\n",
    "\n",
    "Taking the derivative of $E(K(X))$ in the above expression, we get\n",
    "\n",
    "$$\\int_a^b K(x)\\left(p'K(x)+q'\\right)exp[G(x;\\theta)] dx = \\frac{p''q'-q''p'}{(p')^2}. $$\n",
    "\n",
    "Since variance of $K(X)$, namely $V(K(X)) = E([K(X)]^2) - [E(K(X))]^2,$ by using the derived expression for $E(K(X))$ and the above equation, we get $$V(K(X)) = \\frac{p''q'-q''p'}{(p')^3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133959f",
   "metadata": {},
   "source": [
    "**Exercise 7.5.8.** If $X_1, X_2, \\cdots , X_n$ is a random sample from a distribution that has a pdf which is a regular case of the exponential class, show that the pdf of $Y_1 = \\sum_1^n K(X_i)$ is of the form $f_{Y_1} (y_1; \\theta) = R(y_1) \\exp[p(\\theta)y_1 + nq(\\theta)].$\n",
    "\n",
    "*Hint*: Let $Y_2 = X_2,\\cdots,Y_n = X_n$ be $n − 1$ auxiliary random variables. Find the joint pdf of $Y_1,Y_2,\\cdots,Y_n$ and then the marginal pdf of $Y_1$.\n",
    "\n",
    "**Solution:** I could not even figure out how to go about this as I was thinking in terms of completeness and sufficiency. We actually need to go back to basics, and simply apply transformation of variables as shown here.\n",
    "\n",
    "https://math.stackexchange.com/a/4968167/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d7653",
   "metadata": {},
   "source": [
    "**7.5.9.** Let $Y$ denote the median and let $\\overline{X}$ denote the mean of a random sample of size $n=2k+1$ from a distribution that is $N(\\mu,\\sigma^2)$. Compute $E(Y|\\overline{X}=x).$\n",
    "\n",
    "Hint: See Exercise $7.5.4.$\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "There are related posts in the links below\n",
    "\n",
    "https://stats.stackexchange.com/q/83805/183497\n",
    "\n",
    "https://math.stackexchange.com/q/656267/145325\n",
    "\n",
    "I just thought of adding my thoughts here especially because author suggests us to use his hint.\n",
    "\n",
    "According to Theorem $7.3.1$, an unbiased function of a sufficient statistic is also sufficient. So defining $\\phi(\\overline{X}) = E(Y|\\overline{X}),$ if $E(\\phi(\\overline{X})) = E(Y) = \\mu$, and as $\\overline{X}$ is a complete sufficient statistic of $\\mu$, then (following the arguement on the lines of the text after Example $7.4.1$, pg. $432$ and theorem $7.4.1$) we could argue that $E(Y|\\overline{X}=\\overline{x}) = \\phi(\\overline{x}) = \\overline{x}$ almost surely simply because $\\psi(\\overline{X}) = \\overline{X}$ is another function of the statistic $\\overline{X}$ (along with $\\phi(\\overline{X})$) such that $E(\\psi(\\overline{X})) = \\mu$.\n",
    "\n",
    "It remains to be shown that $E(\\phi(\\overline{X})) = E(Y) = \\mu$. Since the number of samples $n=2k+1$ is odd, the $(k+1)^{st}$ order stastic, $Y_{k+1}$, is the median, namely $Y$, and its pdf is given (using equation $4.4.2$) by\n",
    "\n",
    "$$g_{k+1}(y_{k+1}) = \\frac{(2n+1)!}{(n!)^2}\\left[F(y_{k+1})\\right]^k \\left[1-F(y_{k+1})\\right]^k f(y_{k+1}).$$\n",
    "\n",
    "Now the pdf of $N(\\mu,\\sigma^2)$ is symmetric about $y_{k+1}=\\mu$, and is such that $F(\\mu+y_{k+1})+F(\\mu-y_{k+1})=1$. Using that, and the expression above, we can show that even $g_{k+1}(y_{k+1})$ is symmetric about $y_{k+1}=\\mu$ which implies that $E(Y_{k+1}) = E(Y) = \\mu.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e65328",
   "metadata": {},
   "source": [
    "**Exercise 7.5.12.** Let $X_1, X_2, \\cdots , X_n$ be a random sample from a distribution with pmf $p(x;\\theta)=\\theta^x(1−\\theta)$, $x=0,1,2,\\cdots,$ zero elsewhere, where $0\\leq \\theta \\leq 1$.\n",
    "\n",
    "(a) Find the mle, $\\hat{\\theta}$, of $\\theta$.\n",
    "\n",
    "(b) Show that $\\sum_1^n X_i$ is a complete sufficient statistic for $\\theta$.\n",
    "\n",
    "(c) Determine the MVUE of $\\theta$.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(a) MLE turns out to be $Y/(Y+n)$ where $Y=\\sum_1^n X_i$,\n",
    "\n",
    "(b) $Y$ is the complete sufficient statistic for $\\theta$ as this geometric distribution belongs to the exponential family with $K(X)=X$ (we know $\\sum_1^n K(X_i)$ is a complete sufficient statistic for $\\theta$ for exponential family of distributions).\n",
    "\n",
    "(c) The given form of Geometric distribution is the number of attempts it takes before we see the first failure where $x=0$ is also in the support. Then the distribution of $Y=\\sum_1^n X_i$ turns out to be that of negative binomial and is given below.\n",
    "\n",
    "$$P(Y=y) = {y+n-1 \\choose n-1} (1-\\theta)^n\\theta^y, ~~ y=0,1,2,\\cdots$$\n",
    "\n",
    "and as they add up to $1$, we get\n",
    "\n",
    "$$\\sum_{y=0}^\\infty {y+n-1 \\choose n-1} (1-\\theta)^n\\theta^y = 1$$ or \n",
    "$$\\sum_{y=0}^\\infty {y+n-1 \\choose n-1} \\theta^y = \\frac{1}{(1-\\theta)^n}.$$\n",
    "\n",
    "This being a convergent power series, we can integrate once w.r.t $\\theta$ between $0$ and $\\theta$. After some algebra, we arrive at \n",
    "\n",
    "$$\\frac{1-\\theta}{n-1} = \\sum_{z=0}^{\\infty} {z+n-1 \\choose n-1}\\frac{\\theta^z(1-\\theta)^n}{z+n-1}$$\n",
    "\n",
    "where I changed variable from $y \\to z-1$. \n",
    "\n",
    "This expression tells us that (and we might as well revert back to the variable $Y$) $$E\\left(\\frac{n-1}{Y+n-1}\\right) = 1-\\theta$$ or that $$E\\left(\\frac{Y}{Y+n-1}\\right) = \\theta.$$ So the function $\\phi(Y)=Y/(Y+n-1)$ is unbiased and is a function of the complete sufficient statistic for $\\theta$. Hence by Theorem $7.4.1$ (Lehmann and Scheffe) of the text, the MVUE must be $Y/(Y+n-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75efd87",
   "metadata": {},
   "source": [
    "SE has many posts about this but they took a different approach\n",
    "\n",
    "https://math.stackexchange.com/q/2397241/145325\n",
    "\n",
    "https://math.stackexchange.com/q/2038222/145325\n",
    "\n",
    "https://stats.stackexchange.com/q/401314/183497\n",
    "\n",
    "https://math.stackexchange.com/q/1394769/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7bc45",
   "metadata": {},
   "source": [
    "I posted it on SE as well for proof-verification but folks on SE decided to close the post, and this post has been voted for deletion as well. So I am not sure if the link below will work in the future.\n",
    "\n",
    "https://math.stackexchange.com/q/4968752/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21e10f",
   "metadata": {},
   "source": [
    "Almost uniformly, the approach taken on SE posts is to find an unbiased estimator of $\\theta$, (so perhaps $W = I(X > 0)$) and then finding a function of $Y$, say $\\phi(Y)=E(W|Y)$ will be the required MVUE. This is the most commonly used \"trick\" (Theorem $7.3.1$ (Rao-Blackwell) of the text).\n",
    "\n",
    "In this exercise problem however, Hogg et. al. wanted us to find the MLE first and use the form of MLE to figure out the MVUE as described in the text right after Theorem $7.3.2$\n",
    "\n",
    "*We know from Chapters 4 and 6 that, generally, mles are asymptotically unbiased estimators of $\\theta$. Hence, one way to proceed is to find a sufficient statistic and then find the mle. Based on this, we can often obtain an unbiased estimator that is a function of the sufficient statistic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb38eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
