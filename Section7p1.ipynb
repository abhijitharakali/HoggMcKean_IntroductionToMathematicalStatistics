{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1fa7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "import sympy as sym\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1961551",
   "metadata": {},
   "source": [
    "Exercises $7.1.1$,$7.1.2$,$7.1.3$, and $7.1.4$ are solved in Tomoki's solutions manual. Exercise $7.1.6$, and $7.1.7$ are solved in solutions manual. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184545a6",
   "metadata": {},
   "source": [
    "#### Execrice 7.1.3\n",
    "\n",
    "While Tomoki finds the expectation, we need to find the variance of the estimators. Following Stackexchange answers nicely summarize important results which are relevant.\n",
    "\n",
    "https://math.stackexchange.com/a/102857/145325\n",
    "https://math.stackexchange.com/a/2905494/145325\n",
    "\n",
    "So with $E(Y_r)=r\\theta/(n+1)$, unbiased estimators for a random variable having a $U(0,\\theta)$ distribution will be $(n+1)Y_r/r$ which matches the required results. Also, $$\\begin{align}V\\left (\\frac{(n+1)Y_r}{r}\\right )&=\\frac{(n+1)^2}{r^2}V(Y_r) \\\\ &=\\frac{n+1-r}{r(n+2)}\\theta^2.\\end{align}$$\n",
    "\n",
    "So variances in the $n=3$ case turn out to be $3\\theta^2/5$, $\\theta^2/5$, and $\\theta^2/15$ for $r=1$, $2$, and $3$ respectively. As $Y_n$ is supposed to be the best estimate of $\\theta$, it corroborates with this result where variances monotonically decrease with $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb96381",
   "metadata": {},
   "source": [
    "#### Exercise 7.1.5\n",
    "\n",
    "In Example $7.1.2$ of this section, take $\\mathcal{L}[\\theta, \\delta(y)] = |\\theta − \\delta(y)|$. Show that $R(\\theta, \\delta_1) = 1/5\\sqrt{(2/\\pi)}$ and $R(\\theta, \\delta_2) = |\\theta|$. Of these two decision functions $\\delta_1$ and $\\delta_2$, which yields the smaller maximum risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ad9f3",
   "metadata": {},
   "source": [
    "Now $Y \\sim N(\\theta,1/25)$ so define $Z=5(Y-\\theta)$ so that $Z \\sim N(0,1)$. $$\\begin{align} \\mathcal{L}[\\theta, \\delta_1] &= E(|\\theta-Y|) \\\\ &=\\frac{1}{\\sqrt{2\\pi/25}}\\int_{-\\infty}^{\\infty}|\\theta-y|e^{\\frac{-25(y-\\theta)^2}{2}}dy \\\\ &= \\frac{5}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}\\frac{|z|}{5}e^{-z^2/2}\\frac{dz}{5} \\\\ &= \\frac{1}{5}\\sqrt{\\frac{2}{\\pi}} \\end{align}.$$\n",
    "\n",
    "The second one is very easy as $\\mathcal{L}[\\theta, \\delta_2] = E(|\\theta|) = |\\theta|.$ Since the second one is unbounded in $\\theta$, $\\delta_1$ yields a smaller maximum risk simply because it is constant and hence bounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ed180",
   "metadata": {},
   "source": [
    "#### Exercise 7.1.8\n",
    "\n",
    "Let $X_1, X_2, \\cdots{} , X_n$ denote a random sample from a distribution that is $b(1,\\theta)$, $0 \\leq \\theta \\leq 1$. Let $Y = \\sum_{1}^{n} X_i$ and let $\\mathcal{L}[\\theta,\\delta(y)] = [\\theta − \\delta(y)]^2$. Consider decision functions of the form $\\delta(y) = by$, where $b$ does not depend upon $y$. Prove that $R(\\theta, \\delta) = b^2n\\theta(1 − \\theta) + (bn − 1)^2\\theta^2$. Show that \n",
    "\n",
    "$$\\underset{\\theta}{\\max}R(\\theta, \\delta) = \\cfrac{b^4n^2}{4[b^2n-(bn-1)^2]}$$ provided that the value $b$ is such that $b^2n > (bn − 1)^2$. Prove that $b = 1/n$ does not minimize $\\max_\\theta R(\\theta,\\delta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f8126",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "We just need $E([\\theta-by]^2)$ and we know that $Y \\sim b(n,\\theta)$, which means $E(Y)=n\\theta$, and $E(Y^2) = n\\theta(1-\\theta)+n^2\\theta^2$. Using that info, we can easily derive the required expression for $R(\\theta, \\delta)$. The value of $\\theta$ that maximizes the risk is given by $$\\hat{\\theta}=\\cfrac{b^2n}{2[b^2n-(bn-1)^2]}$$ and that is obtained by differentiating $R(\\theta, \\delta)$ w.r.t $\\theta$ and equating to $0$. Double derivative is negative only when $b^2n > (bn − 1)^2$, and plugging that value of $\\theta$ into $R$ gives the desired maximum value for the risk. As regards to the value of $b$ that minimizes $R$, it is given by $\\hat{b}=1/\\sqrt{n(n-1)}\\neq1/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9b75a",
   "metadata": {},
   "source": [
    "#### Exercise 7.1.9.\n",
    "\n",
    "Let $X_1,X_2,\\cdots{},X_n$ be a random sample from a Poisson distribution with mean $\\theta > 0$.\n",
    "\n",
    "(a) Statistician A observes the sample to be the values $x_1,x_2,...,x_n$ with sum $y = \\sum x_i$. Find the mle of $\\theta$.\n",
    "\n",
    "(b) Statistician B loses the sample values $x_1, x_2, \\cdots{} , x_n$ but remembers the sum $y_1$ and the fact that the sample arose from a Poisson distribution. Thus B decides to create some fake observations, which he calls $z_1, z_2,\\cdots{}, z_n$ (as he knows they will probably not equal the original $x$-values) as follows. He notes that the conditional probability of independent Poisson random variables $Z_1,Z_2,\\cdots{},Z_n$ being equal to $z_1,z_2,\\cdots{},z_n$, given $\\sum z_i = y_1$, is\n",
    "\n",
    "$$\\cfrac{ \\frac{\\theta^{z_1}e^{-\\theta}}{z_1!} \\frac{\\theta^{z_2}e^{-\\theta}}{z_2!} \\cdots{} \\frac{\\theta^{z_n}e^{-\\theta}}{z_n!} }{\\frac{(n\\theta)^{y_1}e^{-n\\theta}}{y_1!}} = \\frac{y_1!}{z_1! z_2! \\cdots{} z_n!} \\left ( \\frac{1}{n}\\right )^{z_1} \\left ( \\frac{1}{n}\\right )^{z_2} \\cdots{} \\left ( \\frac{1}{n}\\right )^{z_n}$$\n",
    "\n",
    "since $Y_1 = \\sum Z_i$ has a Poisson distribution with mean $n\\theta$. The latter distribution is multinomial with $y_1$ independent trials, each terminating in one of $n$ mutually exclusive and exhaustive ways, each of which has the same probability $1/n$. Accordingly, B runs such a multinomial experiment $y_1$ independent trials and obtains $z_1, z_2, \\cdots{} , z_n$. Find the likelihood function using these $z$-values. Is it proportional to that of statistician A?\n",
    "\n",
    "$\\textit{Hint}$: Here the likelihood function is the product of this conditional pdf and the pdf of $Y_1  \\sum Z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7468d",
   "metadata": {},
   "source": [
    "Basically $y_1$ is Poisson distributed with parameter $n\\theta$. Also, $x_i$ are Poisson-distributed with parameter $\\theta$. As $\\sum x_i = y$, we have $$L_A = \\frac{\\theta^ye^{-n\\theta}}{\\prod_{i=1}^{n} x_i!}$$ as each term in the product for the likelihood function is of the form $\\frac{\\theta^{x_i}e^{-\\theta}}{x_i!}$ and $\\sum x_i = y$.\n",
    "\n",
    "For statistician B, the given probability has to be multiplied with that of $P(Y_1=y_1)$ and we know that $y_1$ is Poisson distributed with parameter $n\\theta$. So\n",
    "$$L_B = \\frac{y_1!}{\\prod_{i=1}^{n} z_i!}\\left(\\frac{1}{n}\\right)^{y_1} \\frac{(n\\theta)^{y_1}e^{-n\\theta}}{y_1!}. $$ \n",
    "\n",
    "Knowing $y=y_1$, the \"kernel\" in the liklihood functions is the same namely $\\theta^ye^{-n\\theta}$ when we look at the likelihood functiona as being the function of the parameter $\\theta$. So we maximize the likelihood function, we'd end up with the same argmax value $\\hat{\\theta}$ in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6cc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
