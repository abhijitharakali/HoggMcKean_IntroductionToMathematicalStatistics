{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9547551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d4c4a",
   "metadata": {},
   "source": [
    "#### Exercise 5.4.1\n",
    "\n",
    "Let $\\{Xn\\}$ be a sequence of p-dimensional random vectors. Show that $X_n \\xrightarrow{D} N_p(\\mu, \\Sigma)$ if and only if $a′X_n \\xrightarrow{D} N_1(a′\\mu, a′ \\Sigma a)$, for all vectors $a \\in \\mathbb{R}^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1241c8",
   "metadata": {},
   "source": [
    "Solutions manual has the solution for one direction. \n",
    "\n",
    "In the forward direction, we know from Theorem $5.4.3$ that $E(e^{t'X_n}) \\to e^{t'\\mu +t'\\Sigma t/2}$. Here, $t$ is a vector. Let $a \\in \\mathbb{R}^p$ be some vector. Consider the mgf of $X_n$ for the vector parameter $ta$ where $t$ is now a scalar. The mgf $E(e^{ta'X_n})$ converges to  $e^{ta'\\mu +t^2a'\\Sigma a/2}$, ie it is exactly the mgf of $a'X_n$ evaluated at $t$ having a distribution of $N_1(a′\\mu, a′ \\Sigma a)$. Since convergence in mgf is equivalent to convergence in distribution, we have the desired conclusion.\n",
    "\n",
    "In the opposite direction, we are basically given that $a′X_n \\xrightarrow{D} N_1(a′\\mu, a′ \\Sigma a)$, for all vectors $a \\in \\mathbb{R}^p$. Suppose we want to evaluate mgf of $X_n$ ie we want $E(e^{t'X_n})$ for some vector $t \\in \\mathbb{R}^p$. Choose $a=t$, and evaluate the mgf of $a'X_n$ at $1$. Since we are given that $a′X_n \\xrightarrow{D} N_1(a′\\mu, a′ \\Sigma a)$, its mgf converges to that of Normal distribution as well so that we can conclude $E(e^{t'X_n}) \\to e^{t'\\mu +t'\\Sigma t/2}$. This is mgf of $X_n$ evaluated at $t$ so we are done.\n",
    "\n",
    "It'd be a crime not to mention about Cramer-Wold theorem which generalizes the above and the proof simply involves characteristic equation rather than the mgf.\n",
    "\n",
    "$\\textbf{Cramer-Wold Theorem:}$ The distribution of a random $n$-dimensional vector $X$ is completely determined by the set of all one-dimensional distributions of linear combinations $t'X$, where $t$ ranges over all fixed $n$-dimensional vectors ie $\\forall t \\in \\mathbb{R}^n$.\n",
    "\n",
    "See https://math.stackexchange.com/q/3809907/145325 for proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e7c28",
   "metadata": {},
   "source": [
    "#### Exercise 5.4.2. \n",
    "\n",
    "Let $X_1,...,X_n$ be a random sample from a uniform$(a,b)$ distribution. Let $Y_1 = \\textrm{min} X_i$ and let $Y_2 = \\textrm{max} X_i.$ Show that $(Y_1,Y_2)′$ converges in probability to the vector $(a, b)′.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b3945",
   "metadata": {},
   "source": [
    "As usual, we use equation $(4.4.2)$. Distribution of $Y_1$ ends up being $((b-y_1)/(b-a))^n$ and that of $Y_2$ ends up being $((y_2-a)/(b-a))^n$. So $Y_1 \\xrightarrow{D} a$ and $Y_2 \\xrightarrow{D} b$ which gives the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a05c5d",
   "metadata": {},
   "source": [
    "#### Exercise 5.4.3\n",
    "\n",
    "Let $X_n$ and $Y_n$ be p-dimensional random vectors. Show that if $X_n − Y_n \\xrightarrow{P} 0$ and $X_n \\xrightarrow{D} X$ , where $X$ is a p-dimensional random vector, then $Y_n \\xrightarrow{D} X.$\n",
    "\n",
    "The idea is to mimic the proof of Theorem $5.2.3$ whose proof is given in the solutions to section $5.2$ by Prof. Wilkens which is in the Solutions folder. Since this involves the vector norm, we will need Lemma $5.4.1$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f16166",
   "metadata": {},
   "source": [
    "#### Exercise 5.4.4. \n",
    "\n",
    "Let $X_n$ and $Y_n$ be p-dimensional random vectors such that $X_n$ and $Y_n$ are independent for each $n$ and their mgfs exist. Show that if $X_n \\xrightarrow{D} X$ and $Y_n \\xrightarrow{D} Y,$ where $X$ and $Y$ are p-dimensional random vectors, then $(X_n, Y_n) \\xrightarrow{D} (X, Y).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2fd0e",
   "metadata": {},
   "source": [
    "This is actually a very straightforward exercise where we take mgf of $(X_n,Y_n)$ to show that it converges to $(X,Y)$. The caveat is that $X$ and $Y$ should be independent of each other. I think this question is wrong because it does not mention $X$ and $Y$ are independent.\n",
    "\n",
    "See https://math.stackexchange.com/a/3198121/145325 and https://math.stackexchange.com/a/3109720/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21305a",
   "metadata": {},
   "source": [
    "#### Exercise 5.4.5 \n",
    "\n",
    "Suppose $X_n$ has a $N_p(\\mu_n, \\Sigma_n)$ distribution. Show that $X_n \\xrightarrow{D} N_p(\\mu,\\Sigma)$ iff $\\mu_n \\to \\mu$ and $\\Sigma_n \\to \\Sigma .$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4dd20",
   "metadata": {},
   "source": [
    "In the backward direction, Theorem $5.4.3$ tells us that convergence in mgf implies convergence in distribution of the sequence of random vectors. Since the mgf of the convergent seqeunce is that of normal distribution, we have the desired result.\n",
    "The forward direction is actually a bit involved. Proof can be found here https://math.stackexchange.com/a/692533/145325 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf76f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
